# -*- coding: utf-8 -*-
"""Heart_failure_logistic_svm_decisiontree with solution .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oSRd-06ROn2iwSXgZ81ZCtwjkbteEZwP

# Task - 2 (Heart Failure Prediction)

Dataset Link: https://www.kaggle.com/andrewmvd/heart-failure-clinical-data


or by clicking below link
https://raw.githubusercontent.com/sameerCoder/ML_Datasets/main/heart_failure_clinical_records_dataset.csv

Perform data analysis and use different machine learning algorithms to create a model for predicting mortality 
caused by Heart Failure.

Compare at least 3 different algorithms and show their accuracies with the help of a graph.
"""

import warnings
warnings.simplefilter('ignore')

"""# Importing required libraries"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

"""# Loading the dataset"""

filepath ="https://raw.githubusercontent.com/sameerCoder/ML_Datasets/main/heart_failure_clinical_records_dataset.csv"
data = pd.read_csv(filepath)

"""# Performing EDA"""

data.sample(10)

data.shape

data.info()

data.describe()

"""# Checking the outcome labels"""

data['DEATH_EVENT'].value_counts()

plt.figure(figsize=(7, 5))
sns.countplot(data=data, x='DEATH_EVENT')
plt.show()

"""# Checking for null values"""

data.isnull().sum().any()

"""# Checking for duplicate rows"""

duplicated_rows = data[data.duplicated()]
duplicated_rows.shape

"""# Checking the distribution of data """

data.hist(figsize=(15, 15))
plt.show()

"""# Checking Correlation between Dependent and Independent variables"""

plt.figure(figsize = (10, 6))
cormat = sns.heatmap(data.corr(), annot=True)
cormat.set_title('Correlation Matrix')
plt.show()

"""# Outlier Detection and Removal"""

def diagnostic_plot(data, col):
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 3, 1)
    sns.distplot(data[col], bins=10)
    plt.title('Histogram')
    
    plt.subplot(1, 3, 2)
    stats.probplot(data[col], dist='norm', fit=True, plot=plt)
    plt.title('Q-Q Plot')
    
    plt.subplot(1, 3, 3)
    sns.boxplot(y=data[col])
    plt.title('Boxplot')
    
    plt.show()

"""Checking the 'age' column """

data['age'].value_counts()

print("Maximum Age is: {}".format(data['age'].max()))
print("Minimum Age is: {}".format(data['age'].min()))

"""Checking the 'creatinine_phosphokinase' column"""

diagnostic_plot(data, 'creatinine_phosphokinase')

"""Checking the 'ejection_fraction' column"""

diagnostic_plot(data, 'ejection_fraction')

"""Checking the 'platelets' column """

data['platelets'].value_counts().sort_index(ascending=False)

"""Checking the 'serum_creatinine' column """

diagnostic_plot(data, 'serum_creatinine')

"""Checking the 'serum_sodium' column """

diagnostic_plot(data, 'serum_sodium')

"""Checking the 'time' column """

data['time'].value_counts().sort_index(ascending=False)

diagnostic_plot(data, 'time')

data.shape

"""# Seperating Dependent and Independent variables"""

X = data.drop(['DEATH_EVENT'], axis=1)
y = data['DEATH_EVENT']

"""# Performing train-test split"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)

"""# Scaling the data"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

"""# Checking the Accuracy Scores for 3 different models"""

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

from sklearn.pipeline import Pipeline
pipeline_lr = Pipeline([('lr', LogisticRegression())])
pipeline_svc = Pipeline([('svc', SVC())])
pipeline_dt = Pipeline([('dt', DecisionTreeClassifier())])

pipelines = [pipeline_lr, pipeline_svc, pipeline_dt]
best_acc = 0
best_clf = 0
best_pipeline=""
pipe_dict = {0: 'Logistic Regression', 1: 'Support Vector Classifier', 2: 'Decision Tree Classifier'}

for pipe in pipelines:
    pipe.fit(X_train, y_train)
    
for i, model in enumerate(pipelines):
    print("{} - Test Accuracy: {}".format(pipe_dict[i], model.score(X_test, y_test)))
    
for i, model in enumerate(pipelines):
    if model.score(X_test, y_test)>best_acc:
        best_acc = model.score(X_test, y_test)
        best_pipeline = model
        best_clf = i
print("Classifier with best accuracy is {}". format(pipe_dict[best_clf]))

"""# Importing Performance Metrics for Classification"""

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, auc

"""# Logistic Regression"""

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(X_train, y_train)

print("Train accuracy :{}".format(accuracy_score(y_train, lr.predict(X_train))))
print("Test accuracy :{}".format(accuracy_score(y_test, lr.predict(X_test))))

y_pred_lr = lr.predict(X_test)

y_pred_proba_lr = lr.predict_proba(X_test)[:, 1]

sns.heatmap(confusion_matrix(y_test, y_pred_lr), annot=True)
plt.title("Confusion Matrix")
plt.show()

print("Classification Report")
print(classification_report(y_test, y_pred_lr))

print("AUC Score: {}".format(roc_auc_score(y_test, y_pred_proba_lr)))

from sklearn.model_selection import cross_val_score
lr_acc = np.mean(cross_val_score(lr, X, y, cv=10, scoring='accuracy')) 
print("Cross Validation accuracy: {}".format(lr_acc))

"""# Support Vector Classifier"""

from sklearn.svm import SVC
svc = SVC(probability=True)
svc.fit(X_train, y_train)

print("Train accuracy :{}".format(accuracy_score(y_train, svc.predict(X_train))))
print("Test accuracy :{}".format(accuracy_score(y_test, svc.predict(X_test))))

y_pred_svc = svc.predict(X_test)

y_pred_proba_svc = svc.predict_proba(X_test)[:, 1]

sns.heatmap(confusion_matrix(y_test, y_pred_svc), annot=True)
plt.title("Confusion Matrix")
plt.show()

print("Classification Report")
print(classification_report(y_test, y_pred_svc))

print("AUC Score: {}".format(roc_auc_score(y_test, y_pred_proba_svc)))

from sklearn.model_selection import cross_val_score
svc_acc = np.mean(cross_val_score(svc, X, y, cv=10, scoring='accuracy')) 
print("Cross Validation accuracy: {}".format(svc_acc))

"""# Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)

print("Train accuracy :{}".format(accuracy_score(y_train, dt.predict(X_train))))
print("Test accuracy :{}".format(accuracy_score(y_test, dt.predict(X_test))))

y_pred_dt = dt.predict(X_test)

y_pred_proba_dt = dt.predict_proba(X_test)[:, 1]

sns.heatmap(confusion_matrix(y_test, y_pred_dt), annot=True)
plt.title("Confusion Matrix")
plt.show()

print("Classification Report")
print(classification_report(y_test, y_pred_dt))

print("AUC Score: {}".format(roc_auc_score(y_test, y_pred_proba_dt)))

from sklearn.model_selection import cross_val_score
dt_acc = np.mean(cross_val_score(dt, X, y, cv=10, scoring='accuracy')) 
print("Cross Validation accuracy: {}".format(dt_acc))

"""# Plotting ROC Curve for all 3 models to compare their accuracies """

fpr_lr, tpr_lr, threshold_lr = roc_curve(y_test, y_pred_proba_lr)
fpr_svc, tpr_svc, threshold_svc = roc_curve(y_test, y_pred_proba_svc)
fpr_dt, tpr_dt, threshold_dt = roc_curve(y_test, y_pred_proba_dt)

plt.style.use('seaborn-whitegrid')
plt.figure(figsize=(8, 5))
plt.plot(fpr_lr, tpr_lr, label="Logistic Regression")
plt.plot(fpr_svc, tpr_svc, label="Support Vector Classifier")
plt.plot(fpr_dt, tpr_dt, label="Decision Tree Classifier")

plt.legend(loc='lower right', frameon=True)
plt.title("ROC Curve")
plt.ylabel("TPR")
plt.xlabel("FPR")
plt.show()

"""# Tuning the Hyperparameter 'C' for Logistic Regression"""

from sklearn.model_selection import GridSearchCV

params = {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]}

grid = GridSearchCV(estimator=lr, param_grid=params, cv=10, scoring='accuracy', n_jobs=-1)
grid.fit(X,y)

grid.best_params_

grid.best_score_

"""# Tuning the Hyperparameters 'C', 'kernel' & 'degree' for Support Vector Classifier"""

from sklearn.model_selection import RandomizedSearchCV

params = { 'C': [1, 10, 100, 1000],
           'kernel': ['poly'],
           'degree': [2, 3, 4],
         }

random = RandomizedSearchCV(svc, param_distributions=params, cv=10, scoring='accuracy', n_jobs=-1)
random.fit(X,y)

random.best_params_

random.best_score_

"""# Tuning the Hyperparameters for Decision Tree"""

from sklearn.model_selection import GridSearchCV

params = { 'max_depth': [3, 4, 5],
           'min_samples_split': [2, 3],
           'min_samples_leaf': [1, 2, 3]}

grid = GridSearchCV(estimator=dt, param_grid=params, cv=10, scoring='accuracy', n_jobs=-1)
grid.fit(X,y)

grid.best_params_

grid.best_score_

"""# Save the best Model"""

import pickle
pickle.dump(lr, open('model.pkl', 'wb'))
model = pickle.load(open('model.pkl', 'rb'))
print(model)

# predict the output
y_pred_lr = lr.predict(X_test)

# confusion matrix
print('Confusion matrix of Logistic Regression model: \n', confusion_matrix(y_test, y_pred_lr),'\n')

# show the accuracy
print('Accuracy of Logistic Regression  model = ', format(lr_acc))

"""# Thank you"""